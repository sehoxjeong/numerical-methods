{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ee0eb8e",
   "metadata": {},
   "source": [
    "# JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613a702e",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c879a11",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ad2dd8",
   "metadata": {},
   "source": [
    "You should have installed `jax`. If not, please now do so by executing the command:\n",
    "\n",
    "```pip install jaxlib jax```\n",
    "\n",
    "or use Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f86389",
   "metadata": {},
   "source": [
    "### Why and when should you use JAX?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991acf5e",
   "metadata": {},
   "source": [
    "It is good to use JAX if your project requires any of \n",
    "- Python is your primary languqge;\n",
    "- Just-In-Time (JIT) compilation for fast computation;\n",
    "- Automatic differentiation;\n",
    "- Vectorization;\n",
    "- Multiple GPU/TPU acceleration, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eeecca",
   "metadata": {},
   "source": [
    "### Easy to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780604ee",
   "metadata": {},
   "source": [
    "JAX is like a sum of multiple friendly libraries in Python. That is, JAX = numpy + scipy + essential functionalities (e.g., auto-diff, JIT, and parallelization). If you use `numpy` then you should have no problem to use `jax`.\n",
    "\n",
    "<center>\n",
    "\n",
    "|NumPy|JAX|\n",
    "|--|--|\n",
    "|`import numpy as np`|`import jax.numpy as jnp`|\n",
    "|`np.array()`|`jnp.array()`|\n",
    "|`np.zeros()`|`jnp.zeros()`|\n",
    "|`np.ones()`|`jnp.ones()`|\n",
    "|`np.eye()`|`jnp.eye()`|\n",
    "|`np.sin()`|`jnp.sin()`|\n",
    "|`np.linalg`|`jnp.linalg`|\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd4e023",
   "metadata": {},
   "source": [
    "### Galleries: Some Motivating Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf9aac",
   "metadata": {},
   "source": [
    "#### Auto-Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f371efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.5403023, dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def f(x):\n",
    "    return x * jnp.sin(x) + jnp.cos(x)\n",
    "\n",
    "grad_of_f = jax.grad(f) # jax.grad returns a function\n",
    "grad_of_f(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb9f1f1",
   "metadata": {},
   "source": [
    "You say you can compute the gradient by hand? How about something deep ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb9c8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0., dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def something_deep(x):\n",
    "    for i in range(100):\n",
    "        x = f(x)\n",
    "    return x\n",
    "\n",
    "grad_of_deep = jax.grad(something_deep)\n",
    "grad_of_deep(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ed9419",
   "metadata": {},
   "source": [
    "#### JIT Speed Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52058635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax_np(x):\n",
    "    \"\"\"\n",
    "    Softmax in NumPy\n",
    "    \"\"\"\n",
    "    z = np.exp(x)\n",
    "    return z / np.sum(z, axis=0)\n",
    "\n",
    "@jax.jit\n",
    "def softmax_jax(x):\n",
    "    \"\"\"\n",
    "    Softmax in JAX\n",
    "    \"\"\"\n",
    "    z = jnp.exp(x)\n",
    "    return z / jnp.sum(z, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9f8aa29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.1 μs ± 223 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "x = np.ones((10, 1000))\n",
    "%timeit softmax_np(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82763d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.1 μs ± 31.5 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "x = jnp.ones((10, 1000))\n",
    "%timeit softmax_jax(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a359ba3e",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dfe68b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44deacb",
   "metadata": {},
   "source": [
    "JAX by default uses `float32` for the best compatibility with GPUs. You need to manually do the following to enable `float64` globally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32dd6118",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update('jax_enable_x64', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "46bc644d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecd65cf",
   "metadata": {},
   "source": [
    "## JIT in JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e13cee",
   "metadata": {},
   "source": [
    "Suppose that we have a function that is giga-computational demanding.\n",
    "\n",
    "```python\n",
    "def expensive_func(x):\n",
    "    # a bunch of giga-heavy computations\n",
    "    # Python is slow, and NumPy too\n",
    "    return ...\n",
    "```\n",
    "\n",
    "JAX can make the function above faster by compiling it into machine codes by using the JIT compilation. We trigger the **compilation** by calling `jax.jit`.\n",
    "\n",
    "```python\n",
    "import jax\n",
    "jitted_func = jax.jit(expensive_func)\n",
    "```\n",
    "\n",
    "Note the return `jitted_func` is a function that has exactly the same signature as `expensive_func`. It is equivalent to put a decorator over the function definition.\n",
    "\n",
    "```python\n",
    "@jax.jit\n",
    "def expensive_func(x):\n",
    "    ...\n",
    "    return ...\n",
    "```\n",
    "\n",
    "We begin with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dec9d9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_np(x):\n",
    "    return np.diff(np.diff(np.diff(np.diff(np.diff(x)))))\n",
    "\n",
    "@jax.jit\n",
    "def func_jax(x):\n",
    "    return jnp.diff(jnp.diff(jnp.diff(jnp.diff(jnp.diff(x)))))\n",
    "\n",
    "x = np.random.randn(100_000)\n",
    "xj = jnp.asarray(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db8aba29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.5 μs ± 2.49 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit func_np(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60a1d327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger JIT\n",
    "# func_jax(xj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea3c9173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.8 μs ± 594 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit func_jax(xj).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06540398",
   "metadata": {},
   "source": [
    "Around 2 times faster. NumPy uses OpenBLAS/MKL/BLIS... as backend. Why is it still slow? A more illustrative example which shows that the overhead of NumPy is problematic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcb37267",
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B = np.eye(50), np.eye(50)\n",
    "\n",
    "def func_np(x):\n",
    "    for i in range(100):\n",
    "        x = B @ x + np.linalg.solve(A, x) + np.linalg.norm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fdd5ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JAX version of the code above\n",
    "Aj, Bj = jnp.eye(50), jnp.eye(50)\n",
    "\n",
    "@jax.jit\n",
    "def func_jax(x):\n",
    "    def scan_body(carry, _):\n",
    "        x = carry\n",
    "        return Bj @ x + jnp.linalg.solve(Aj, x) + jnp.linalg.norm(x), _\n",
    "    return jax.lax.scan(scan_body, x, jnp.arange(100))[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d8c79a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.59 ms ± 40.1 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit func_np(np.ones((50, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5360cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 μs ± 334 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Trigger JIT\n",
    "# func_jax(jnp.ones((50, )))\n",
    "\n",
    "%timeit func_jax(jnp.ones((50, ))).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3055ba66",
   "metadata": {},
   "source": [
    "### What happened inside JIT?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90ca846",
   "metadata": {},
   "source": [
    "1. When Python executes `func_jax` for the first time, the function `jax.jit` traces and traverses all the operations inside the function `func_jax`.\n",
    "2. Then JIT compiles these operations into the accelerated linear algebra (XLA) codes. Imagine this as that of compiling C codes to an executable file. No numerical operations are done!\n",
    "3. Then, by the next time you call the jitted `func_jax`, Python will execute the compiled XLA codes to carry out the numerical computations.\n",
    "4. After the numerical computations are done in the machine level, the results are sent back to Python.\n",
    "\n",
    "JAX basically uses Python as a **metaprogramming language** that specifies how to build an XLA program (quote by [Patrck Kidger](https://kidger.site/thoughts/jax-vs-julia/))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2b803e",
   "metadata": {},
   "source": [
    "### When and Where to JIT?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63750e0a",
   "metadata": {},
   "source": [
    "Usually, we JIT\n",
    "- the part that has the largest scope, so that the compiler can understand your program better,\n",
    "- or the function(s) that are called repetitively, for instance, the objective function in optimization:\n",
    "```python\n",
    "@jax.jit\n",
    "def objective_func(params):\n",
    "    ...\n",
    "    return ...\n",
    "```\n",
    "\n",
    "Remember, when we write the JAX code, we are describing a computation flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf320d58",
   "metadata": {},
   "source": [
    "### Will these work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8103235e",
   "metadata": {},
   "source": [
    "Note that the following example do not work. Guess why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d858de94",
   "metadata": {},
   "outputs": [
    {
     "ename": "TracerArrayConversionError",
     "evalue": "The numpy.ndarray conversion method __array__() was called on traced array with shape float64[2]\nThe error occurred while tracing the function my_func at /var/folders/n5/bkphn15s0kzc1lsr8x64v8y00000gn/T/ipykernel_1159/153899635.py:1 for jit. This concrete value was not available in Python because it depends on the value of the argument x.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTracerArrayConversionError\u001b[39m                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;129m@jax\u001b[39m.jit\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmy_func\u001b[39m(x):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.exp(x)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mmy_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 14 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mmy_func\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;129m@jax\u001b[39m.jit\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmy_func\u001b[39m(x):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coursework/Macroeconomics/lib/python3.12/site-packages/jax/_src/core.py:761\u001b[39m, in \u001b[36mTracer.__array__\u001b[39m\u001b[34m(self, *args, **kw)\u001b[39m\n\u001b[32m    760\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kw):\n\u001b[32m--> \u001b[39m\u001b[32m761\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m TracerArrayConversionError(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mTracerArrayConversionError\u001b[39m: The numpy.ndarray conversion method __array__() was called on traced array with shape float64[2]\nThe error occurred while tracing the function my_func at /var/folders/n5/bkphn15s0kzc1lsr8x64v8y00000gn/T/ipykernel_1159/153899635.py:1 for jit. This concrete value was not available in Python because it depends on the value of the argument x.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def my_func(x):\n",
    "    return np.exp(x)\n",
    "\n",
    "my_func(jnp.ones((2, )))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a10cb9",
   "metadata": {},
   "source": [
    "Below does not work in some early JAX versions; now, they do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41ff7004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3.]\n",
      "[2. 3.]\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def my_func(x):\n",
    "    return x + np.array([1., 2.])\n",
    "\n",
    "print(my_func(jnp.ones((2, ))))\n",
    "print(my_func(np.ones(2, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71532403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.y = jnp.array(1.)\n",
    "\n",
    "    @jax.jit\n",
    "    def my_method(self, x):\n",
    "        return x + self.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f6189",
   "metadata": {},
   "source": [
    "Notice that the following does not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96acabe9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Error interpreting argument to <function MyClass.my_method at 0x11b1e1300> as an abstract array. The problematic value is of type <class '__main__.MyClass'> and was passed to the function at path self.\nThis typically means that a jit-wrapped function was called with a non-array argument, and this argument was not marked as static using the static_argnums or static_argnames parameters of jax.jit.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m obj = MyClass()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmy_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2.\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 5 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coursework/Macroeconomics/lib/python3.12/site-packages/jax/_src/pjit.py:739\u001b[39m, in \u001b[36m_infer_input_type\u001b[39m\u001b[34m(fun, dbg, explicit_args)\u001b[39m\n\u001b[32m    737\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    738\u001b[39m   arg_description = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpath \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdbg.arg_names[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m739\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    740\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError interpreting argument to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfun\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m as an abstract array.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    741\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m The problematic value is of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and was passed to\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    742\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m the function at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_description\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    743\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThis typically means that a jit-wrapped function was called with a non-array\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    744\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m argument, and this argument was not marked as static using the\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    745\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m static_argnums or static_argnames parameters of jax.jit.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    746\u001b[39m   ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.mutable_array_checks.value:\n\u001b[32m    748\u001b[39m   _check_no_aliased_ref_args(dbg, avals, explicit_args)\n",
      "\u001b[31mTypeError\u001b[39m: Error interpreting argument to <function MyClass.my_method at 0x11b1e1300> as an abstract array. The problematic value is of type <class '__main__.MyClass'> and was passed to the function at path self.\nThis typically means that a jit-wrapped function was called with a non-array argument, and this argument was not marked as static using the static_argnums or static_argnames parameters of jax.jit."
     ]
    }
   ],
   "source": [
    "obj = MyClass()\n",
    "obj.my_method(jnp.array(2.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8982f7f0",
   "metadata": {},
   "source": [
    "We can force it to work by adding a `static argnums` option. This assumes that `self` is static, i.e., immutable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b00f5c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "198c0277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.y = jnp.array(1.)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0, ))\n",
    "    def my_method(self, x):\n",
    "        return x + self.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "161f18fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(3., dtype=float64, weak_type=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = MyClass()\n",
    "obj.my_method(jnp.array(2.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbb57a9",
   "metadata": {},
   "source": [
    "Similarly, this will not (immediately) work either:\n",
    "\n",
    "```python\n",
    "@jax.jit\n",
    "def f(x, g: Callable):\n",
    "    return x + g(x)\n",
    "```\n",
    "\n",
    "We have to make it clear that the argument `g` is static:\n",
    "\n",
    "```python\n",
    "@partial(jax.jit, static_argnums=(1, ))\n",
    "def f(x, g: Callable):\n",
    "    return x + g(x)\n",
    "```\n",
    "\n",
    "A more concise way is to put `g` in an outer scope.\n",
    "\n",
    "```python\n",
    "g = ... # definition of g from outer scope\n",
    "\n",
    "@jax.jit\n",
    "def f(x):\n",
    "    return x + g(x)\n",
    "```\n",
    "\n",
    "Note that whenever the static argument changes, it will trigger the JIT compilation again to create another XLA code. JAX accepts immutable objects, while Python Class is mutable. For instance,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e5b9392",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "JAX arrays are immutable and do not support in-place item assignment. Instead of x[idx] = y, use x = x.at[idx].set(y) or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m x = jnp.ones((\u001b[32m2\u001b[39m, ))\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m = \u001b[32m1.\u001b[39m \u001b[38;5;66;03m# This does not work.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coursework/Macroeconomics/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:592\u001b[39m, in \u001b[36m_unimplemented_setitem\u001b[39m\u001b[34m(self, i, x)\u001b[39m\n\u001b[32m    588\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_unimplemented_setitem\u001b[39m(\u001b[38;5;28mself\u001b[39m, i, x):\n\u001b[32m    589\u001b[39m   msg = (\u001b[33m\"\u001b[39m\u001b[33mJAX arrays are immutable and do not support in-place item assignment.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    590\u001b[39m          \u001b[33m\"\u001b[39m\u001b[33m Instead of x[idx] = y, use x = x.at[idx].set(y) or another .at[] method:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    591\u001b[39m          \u001b[33m\"\u001b[39m\u001b[33m https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg.format(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)))\n",
      "\u001b[31mTypeError\u001b[39m: JAX arrays are immutable and do not support in-place item assignment. Instead of x[idx] = y, use x = x.at[idx].set(y) or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html"
     ]
    }
   ],
   "source": [
    "x = jnp.ones((2, ))\n",
    "x[0] = 1. # This does not work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2fdd7f",
   "metadata": {},
   "source": [
    "Well, so far, doesn't JAX suck if we cannot even assign or update values to variables? No, this in my opinion a feature not a problem. Immutable objects are best for computations. For projects that need mutable objects, we can rewrite then into that of based on immutable ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071dca73",
   "metadata": {},
   "source": [
    "## Randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b2cadb",
   "metadata": {},
   "source": [
    "### Generating Random Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b386b873",
   "metadata": {},
   "source": [
    "Generating random numbers in JAX needs the programmer to be conscious, not drunk. This is going to look painful in the beginning. But in the end we will see this is super helpful in controlling the randomness flow. \n",
    "\n",
    "Think about how to generate random samples in NumPy, for example, \n",
    "\n",
    "```python\n",
    "np.random.seed(100)\n",
    "samples1 = np.random.randn(5)\n",
    "samples2 = np.random.uniform(5)\n",
    "samples3 = np.random.gamma(5)\n",
    "```\n",
    "\n",
    "The results are reproducible under a fixed random seed.\n",
    "\n",
    "Then analogously, can we do the following?\n",
    "\n",
    "```python\n",
    "jnp.random.seed(100)\n",
    "```\n",
    "\n",
    "No, we have to manually play with random keys by yourself, like in C, C++, or Rust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d98b98b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The equivalence to the NumPy code\n",
    "\n",
    "key = jax.random.PRNGKey(100)\n",
    "samples1 = jax.random.normal(key, shape=(5, ))\n",
    "\n",
    "key, _ = jax.random.split(key)\n",
    "samples2 = jax.random.uniform(key, shape=(5, ))\n",
    "\n",
    "key, _ = jax.random.split(key)\n",
    "samples3 = jax.random.gamma(key, shape=(5, ), a=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e4d619",
   "metadata": {},
   "source": [
    "We may think explicitly splitting random keys is a fuss/verbose, but this is standard for asynchronous/parallel/vectorizable programs. Recall the NumPy version:\n",
    "```python\n",
    "np.random.seed(100)\n",
    "samples1 = np.random.randn(5)\n",
    "samples2 = np.random.uniform(5)\n",
    "samples3 = np.random.gamma(5)\n",
    "```\n",
    "Why are the results reproducible? Because, (1) the random seed is fixed, and (2) the order of execution is fixed. If `samples1`, `samples2`, and `samples3` are executed in parallel, can you guarantee the results are the same? No, because you will not know the order of execution. This essentially introduces another randomness (at hardware level) that is hard to control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15ff08dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 100]\n",
      "[ 701046466 2104227382] [2213033797 2583756506]\n",
      "[3549270403 1145628597] [1315821631 2840677502]\n",
      "[[2733948186  829389578]\n",
      " [2279201908 4116190116]\n",
      " [1481730141 3233576484]\n",
      " [1130548228  472880600]\n",
      " [4224117979 1514228929]]\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(100)\n",
    "print(key)\n",
    "\n",
    "# The randomness under this splitted key is independent of the previous key.\n",
    "key, subkey = jax.random.split(key)\n",
    "print(key, subkey)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "print(key, subkey)\n",
    "\n",
    "keys = jax.random.split(key, num=5)\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442fb43d",
   "metadata": {},
   "source": [
    "In practice, you can just ignore `subkey` which is used to track randomness:\n",
    "\n",
    "```python\n",
    "key, _ = jax.random.split(key)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd71e353",
   "metadata": {},
   "source": [
    "Imagine that we have an algorithm and we would like to test it on a synthetic model for 100 **independent** Monte Carlo runs then average the results.\n",
    "\n",
    "```python\n",
    "# Example code \n",
    "num_mcs = 100\n",
    "for i in range(num_mcs):\n",
    "    np.random.seed(i)\n",
    "    data = generate_data()\n",
    "    result = my_algorithm(data)\n",
    "```\n",
    "\n",
    "The implementation above is a common mistake, as seeds 1, 2, ..., 100 are not independent. How to make the randomness in the loop independent to each other? I actually don't know how to do so in NumPy.\n",
    "\n",
    "In JAX, it is convenient:\n",
    "\n",
    "```python\n",
    "num_mcs = 100\n",
    "key = jax.random.PRNGKey(1)\n",
    "for i in range(num_mcs):\n",
    "    key, _ = jax.random.split(key)\n",
    "    data = generate_data\n",
    "    result = my_algorithm(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1163311",
   "metadata": {},
   "source": [
    "Let's do a simple exercise: Generate two random positive definite matrices of size 6, then add them. The NumPy-style implementation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b39fa778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.          2.          2.          2.          4.05034118 13.36009279]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(999)\n",
    "\n",
    "rand = np.random.randn(6)\n",
    "psd_matrix_1 = np.outer(rand, rand) + np.eye(6)\n",
    "\n",
    "rand = np.random.randn(6)\n",
    "psd_matrix_2 = np.outer(rand, rand) + np.eye(6)\n",
    "\n",
    "print(np.linalg.eigh(psd_matrix_1 + psd_matrix_2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574c380",
   "metadata": {},
   "source": [
    "Then the JAX version of the code is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a8cc9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.         2.         2.         2.         4.43398354 8.80012971]\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(100)\n",
    "rand = jax.random.normal(key, shape=(6, ))\n",
    "psd_matrix_1 = jnp.outer(rand, rand) + jnp.eye(6)\n",
    "\n",
    "key, _ = jax.random.split(key)\n",
    "rand = jax.random.normal(key, shape=(6, ))\n",
    "psd_matrix_2 = jnp.outer(rand, rand) + jnp.eye(6)\n",
    "\n",
    "print(jnp.linalg.eigh(psd_matrix_1 + psd_matrix_2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efae8396",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b61a6a7",
   "metadata": {},
   "source": [
    "Vectorization is one of the most helpful feature of JAX for economic modeling. The vectorization here is not the same as parallelization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08a78ca",
   "metadata": {},
   "source": [
    "Let's begin with some example. Suppose that we have two arrays `a` and `b` of shapes `(10, 5, 3, 6)` and `(10, 3, 7, 6)`, respectively. Then how do we do `a @ b` in the way that the multiplication applies to `(5, 3) ⨉ (3, 7)` while taking other dimesions as the broadcasting dimension? Eventually, we desire an array of shape `(10, 5, 7, 6)`. How do we do it in NumPy or Matlab? (Recall `einsum`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9043af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c12de8b",
   "metadata": {},
   "source": [
    "Now let's look at another example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f9d091ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x, y):\n",
    "    \"\"\"\n",
    "    Arguments x and y have the same shape (2, ).\n",
    "    Return a (2, 2) matrix.\n",
    "    \"\"\"\n",
    "    z = x * y\n",
    "    return np.array([[x[0] ** 2, x[0] * x[1]], [np.sin(x[1]), x[0] + x[1]]]) + np.outer(z, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5146011",
   "metadata": {},
   "source": [
    "Now if inputs `x` and `y` are of shape `(100, 2)`, how can we batch ober the first dimension and return a tensor `(100, 2, 2)`? Even more complicated, if the inputs `x` and `y` are of shapes `(100, 2)` and `(300, 2)`, respectively, how do we visit over 100 and 300 and return a tensor `(100, 300, 2, 2)`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4db6f0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2, 2)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NumPy version\n",
    "np_vectorized_func = np.vectorize(func, signature='(n), (n) -> (n, n)')\n",
    "np_vectorized_func(np.ones((5,  2)), np.ones((5, 2))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3210bc82",
   "metadata": {},
   "source": [
    "Cool! NumPy has a concise way to do the vectorization. However, please note that `np.vectorize` is merely a syntax sugar of a python loop. It is **not a vectorization on the computation level**. Now let's look at the JAX implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "670be63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def func(x, y):\n",
    "    \"\"\"\n",
    "    Arguments x and y are of the same shape (2, ).\n",
    "    Return a (2, 2) matrix.\n",
    "    \"\"\"\n",
    "    z = x * y\n",
    "    return jnp.array([[x[0] ** 2, x[0] * x[1]], [jnp.sin(x[1]), x[0] + x[1]]]) + jnp.outer(z, z)\n",
    "\n",
    "jax_vectorized_func = jax.vmap(func, in_axes=(0, 0))\n",
    "jax_vectorized_func(jnp.ones((5, 2)), jnp.ones((5, 2))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c05eb83",
   "metadata": {},
   "source": [
    "Let's now compare the speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "87bf86da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.6 ms ± 845 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "a = np.ones((10000, 2))\n",
    "%timeit np_vectorized_func(a, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a962c647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.31 ms ± 8.24 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "a = jnp.asarray(a)\n",
    "_ = jax_vectorized_func(a, a)\n",
    "%timeit jax_vectorized_func(a, a).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d360744",
   "metadata": {},
   "source": [
    "`jax.vmap(func, in_axes) -> Callable[the same signature as func]`\n",
    "- `func`: the function you want to vectorize;\n",
    "- `in_axes`: a tuple/list that indicates the vectorization axes for the arguments of `func`.\n",
    "\n",
    "Recall that our `func(x, y)` takes two `(2, )` arrays as inputs and returns a `(2, 2)` matrix.\n",
    "- To vectorize for `x: (n, 2)` and `y: (n, 2)` for some `n`, use `in_axes=(0, 0)` to get `(n, ...)`.\n",
    "- To vectorize for `x: (2, n)` and `y: (n, 2)` for some `n`, use `in_axes=(1, 0)` to get `(n, ...)`.\n",
    "- To vectorize for `x: (n, 2)` and `y: (2, )` for some `n`, use `in_axes=(0, None)` to get `(n, ...)`.\n",
    "- To vectorize for `x: (m, 2)` and `y: (n, 2)` for some `m` and `n`, use two `vmap` nested to get `(m, n, ...)`, i.e.,\n",
    "```python\n",
    "jax.vmap(jax.vmap(func, in_axes=(0, None)), in_axes=(None, 0))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f3130",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f45f11e",
   "metadata": {},
   "source": [
    "Compute the following Monte Carlo approximation of expectation, $$\\mathbb E[g(X)] \\approx \\frac{1}{N}\\sum^N_{i=1}g\\left(X^{(i)}\\right),$$ where $X, X^{(1)}, X^{(2)}, \\cdots \\sim N(\\mathbf 0, I_2)$ and $$g(X) = \\begin{bmatrix} \n",
    "\\exp(X_1) \\exp(X_2) \\\\\n",
    "X_1 X_2 + X_1\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4127c5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "[ 2.77623283 -0.02325937]\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "N = 1000\n",
    "key = jax.random.PRNGKey(100)\n",
    "samples = jax.random.normal(key, shape=(N, 2))\n",
    "\n",
    "def g(X):\n",
    "    X1, X2 = X\n",
    "    return jnp.array([jnp.exp(X1) * jnp.exp(X2), X1 * X2 + X1])\n",
    "\n",
    "vectorized_g = jax.vmap(g, in_axes=(0, ))\n",
    "propagated_samples = vectorized_g(samples)\n",
    "print(propagated_samples.shape)\n",
    "mean_g = jnp.mean(propagated_samples, axis=0)\n",
    "print(mean_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6ed89c",
   "metadata": {},
   "source": [
    "Next, consider a Matern 3/2 covariance function $C: \\mathbb R \\times \\mathbb R \\rightarrow \\mathbb R$ defined by\n",
    "\n",
    "$$C(t, t') = \\sigma^2 \\left(1 + \\frac{\\sqrt 3 \\lvert t - t' \\rvert}{l}\\right)\\exp\\left( - \\frac{\\sqrt 3 \\lvert t - t' \\rvert}{l} \\right).$$\n",
    "\n",
    "Say, now we have $T$ data points $(t_1, t_2, \\cdots, t_T)$, compute the covariance matrix evaluated at the Cartesian $(t_1, t_2, \\cdots, t_T) \\times (t_1, t_2, \\cdots, t_T)$, that is, $$C_{1:T} = \\begin{bmatrix}\n",
    "C(t_1, t_1) & C(t_1, t_2) & \\cdots & C(t_1, t_T) \\\\\n",
    "C(t_2, t_1) & C(t_2, t_2) & \\cdots & C(t_1, t_1) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "C(t_T, t_1) & C(t_T, t_2) & \\cdots & C(t_T, t_T)\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bf273693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.contour.QuadContourSet at 0x345b635c0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "def cov_func(t1: float, t2: float, l: float, σ: float) -> float:\n",
    "    return (σ ** 2) * (1 + jnp.sqrt(3) * jnp.abs(t1 - t2) / l) * (jnp.exp(-jnp.sqrt(3) * jnp.abs(t1 - t2) / l))\n",
    "\n",
    "vectorized_cov_func = jax.vmap(jax.vmap(cov_func, in_axes=(0, None, None, None)), in_axes=(None, 0, None, None))\n",
    "\n",
    "# Equivalently,\n",
    "# @partial(jax.vmap, in_axes=(None, 0, None, None))\n",
    "# @partial(jax.vmap, in_axes=(0, None, 0, 0))\n",
    "# def cov_func(t1: float, t2: float, l: float, σ: float) -> float:\n",
    "#     return (σ ** 2) * (1 + jnp.sqrt(3) * jnp.abs(t1 - t2) / l) * (jnp.exp(-jnp.sqrt(3) * jnp.abs(t1 - t2) / l))\n",
    "\n",
    "ts = jnp.linspace(0.01, 1, 100)\n",
    "\n",
    "l, σ = 0.1, 1.\n",
    "cov_matrix = vectorized_cov_func(ts, ts, l, σ)\n",
    "\n",
    "plt.contourf(cov_matrix, levels=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76561ebb",
   "metadata": {},
   "source": [
    "Now we compare the speed of NumPy-style and JAX-style code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c29e609e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 ms ± 6.31 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# NumPy implementation 1. Naive implementation with two loops. Do not use this in practice.\n",
    "\n",
    "def np_cov_func(t1, t2):\n",
    "    cov_matrix = np.zeros((ts.size, ts.size))\n",
    "    for i, t1 in enumerate(ts):\n",
    "        for j, t2 in enumerate(ts):\n",
    "            z = np.sqrt(3) * np.abs(t1 - t2) / l\n",
    "            cov_matrix[i, j] = σ**2 * (1 + z) * np.exp(-z)\n",
    "    return cov_matrix\n",
    "\n",
    "ts_np = np.asarray(ts)\n",
    "%timeit np_cov_func(ts_np, ts_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "86758ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.8 μs ± 1.2 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# NumPy implementation 2. Using broadcasting.\n",
    "# While useful, this is applicable only for limited applications.\n",
    "\n",
    "def np_cov_func(t1, t2):\n",
    "    z = np.sqrt(3) * np.abs(t1[:, None] - t2[None, :]) / l\n",
    "    return σ ** 2 * (1 + z) * np.exp(-z)\n",
    "\n",
    "ts_np = np.asarray(ts)\n",
    "%timeit np_cov_func(ts_np, ts_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f3feac9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.9 μs ± 1.65 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# NumPy implementation 3. Using SciPy `cdist`.\n",
    "\n",
    "import scipy.spatial\n",
    "\n",
    "def np_cov_func(t1, t2):\n",
    "    r = scipy.spatial.distance.cdist(t1, t2, 'euclidean')\n",
    "    z = np.sqrt(3) * r / l\n",
    "    return σ ** 2 * (1 + z) * np.exp(-z)\n",
    "\n",
    "ts_np = np.asarray(ts).reshape(-1, 1)\n",
    "%timeit np_cov_func(ts_np, ts_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "32193715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.79 ms ± 73.7 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit vectorized_cov_func(ts, ts, l, σ).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b36cccd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.3 μs ± 503 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# In principle, we should not jit vmap which is jitted already, \n",
    "# but for some reasons the jitted vmap is faster than that of non-jitted in my computer.\n",
    "f = jax.jit(vectorized_cov_func)\n",
    "_ = f(ts, ts, l, σ)\n",
    "\n",
    "%timeit f(ts, ts, l, σ).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d101b",
   "metadata": {},
   "source": [
    "There is also `jax.pmap`. This is for parallelization across different divices, for example, multiple GPUs/TPUs. See details [here](https://docs.jax.dev/en/latest/sharded-computation.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db896313",
   "metadata": {},
   "source": [
    "## Auto-Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeb3909",
   "metadata": {},
   "source": [
    "### Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae29cec",
   "metadata": {},
   "source": [
    "Consider a function $f: \\mathbb R^d \\rightarrow \\mathbb R$. JAX computes the gradient of $f$ by `grad_f = jax.grad(f)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c9ee99",
   "metadata": {},
   "source": [
    "We will practice by implementing examples. Let a function $f$ be defined as $$f(x) = x^\\top A x.$$ Compute $\\nabla f$, and compare with the true gradient $2 A x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0546befc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([2., 4.], dtype=float64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = jnp.eye(2)\n",
    "\n",
    "def f(x):\n",
    "    return jnp.dot(x, A @ x)\n",
    "\n",
    "grad_f = jax.grad(f)\n",
    "grad_f(jnp.array([1., 2.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc41b5",
   "metadata": {},
   "source": [
    "Next, consider an MLE objective function $$\\ell(\\theta) = x^\\top A^{-1}(\\theta)x,$$ where $$A(\\theta) = \\begin{bmatrix} 2 & \\text{sigmoid}(\\theta) \\\\ \\text{sigmoid}(\\theta) & 3 \\end{bmatrix},$$ and $x \\in \\mathbb R^2$ is given. Compute $\\nabla \\ell$ at $\\theta = 2$, and compare the result to that of the finite difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f027eff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] JAX auto-diff: 0.02155437443121922\n",
      "[02] Finite diff  : 0.021554311946392343\n"
     ]
    }
   ],
   "source": [
    "import jax.scipy\n",
    "\n",
    "key = jax.random.PRNGKey(100)\n",
    "x = jax.random.normal(key, shape=(2, ))\n",
    "\n",
    "def sigmoid(θ):\n",
    "    return 1 / (1 + jnp.exp(-θ))\n",
    "\n",
    "def mle_objective(θ):\n",
    "    A = jnp.array([[2., sigmoid(θ)], \n",
    "                   [sigmoid(θ), 3.]])\n",
    "    chol = jax.scipy.linalg.cho_factor(A, lower=True)\n",
    "    return jnp.dot(x, jax.scipy.linalg.cho_solve(chol, x))\n",
    "\n",
    "grad_mle_obj = jax.grad(mle_objective)\n",
    "print('[01] JAX auto-diff:', grad_mle_obj(2.))\n",
    "\n",
    "# Compute gradient at 2. using finite difference.\n",
    "print('[02] Finite diff  :', (mle_objective(2. + 1e-5) - mle_objective(2.)) / 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a63ac",
   "metadata": {},
   "source": [
    "### Jacobian and Hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292d80b3",
   "metadata": {},
   "source": [
    "JAX computes Jacobian of any function $f: \\mathbb R^n \\rightarrow \\mathbb R^m$ by `jax.jacfwd` or `jax.jacrev`. They give the same results but are implemented in different ways:\n",
    "- `jax.jacfwd` uses forward-mode autodiff, while\n",
    "- `jax.jacrev` uses reverse-mode autodiff.\n",
    "\n",
    "I am no expert in autodiff, but essentially, we use `jacfwd` when $n \\ll m$ while we use `jacrev` when $n \\gg m$ for the best computation speed. To obtain the Hessian of a function, we could use either `jacfwd(jacrev(f))` or `jax.hessian(f)`.\n",
    "\n",
    "For example, take the Jacobian of $\\nabla f$ from the first example, where $$f(x) = x^\\top A x.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8ce4bf62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[2., 0.],\n",
       "       [0., 2.]], dtype=float64)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jac_of_gradf = jax.jacfwd(grad_f)\n",
    "\n",
    "# Evaluate at x = (0.1, 0.2)\n",
    "jac_of_gradf(jnp.array((0.1, 0.2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d7878e",
   "metadata": {},
   "source": [
    "Next, consider a simple perceptron $$\\text{NN}(x) = \\text{sigmoid}(w^\\top x + b).$$ Compute its gradient and Hessian with respect to the weight $w$. (*Hint.* Use a parameter `argnums` to specify positional argument(s) to differentiate with respect to.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "08a8c616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.91972490e-01 -1.03381322e-01 -2.31600852e-02 -8.93785534e-02\n",
      " -3.23622463e-01 -3.45593031e-01  2.09718645e-01  1.25725821e-05\n",
      "  1.58379847e-01 -1.47384540e-01]\n",
      "[[-7.72030384e-02  4.15754998e-02  9.31398529e-03  3.59441913e-02\n",
      "   1.30146968e-01  1.38982581e-01 -8.43397750e-02 -5.05614916e-06\n",
      "  -6.36935292e-02  5.92716921e-02]\n",
      " [ 4.15754998e-02 -2.23893026e-02 -5.01578179e-03 -1.93567216e-02\n",
      "  -7.00869466e-02 -7.48451147e-02  4.54187863e-02  2.72284528e-06\n",
      "   3.43003380e-02 -3.19190834e-02]\n",
      " [ 9.31398529e-03 -5.01578179e-03 -1.12366461e-03 -4.33640535e-03\n",
      "  -1.57012854e-02 -1.67672379e-02  1.01749807e-02  6.09987637e-07\n",
      "   7.68416123e-03 -7.15069873e-03]\n",
      " [ 3.59441913e-02 -1.93567216e-02 -4.33640535e-03 -1.67348969e-02\n",
      "  -6.05938263e-02 -6.47075112e-02  3.92669132e-02  2.35404197e-06\n",
      "   2.96544339e-02 -2.75957149e-02]\n",
      " [ 1.30146968e-01 -7.00869466e-02 -1.57012854e-02 -6.05938263e-02\n",
      "  -2.19398531e-01 -2.34293389e-01  1.42177901e-01  8.52353088e-06\n",
      "   1.07372972e-01 -9.99187488e-02]\n",
      " [ 1.38982581e-01 -7.48451147e-02 -1.67672379e-02 -6.47075112e-02\n",
      "  -2.34293389e-01 -2.50199451e-01  1.51830289e-01  9.10218917e-06\n",
      "   1.14662470e-01 -1.06702183e-01]\n",
      " [-8.43397750e-02  4.54187863e-02  1.01749807e-02  3.92669132e-02\n",
      "   1.42177901e-01  1.51830289e-01 -9.21362396e-02 -5.52354533e-06\n",
      "  -6.95814314e-02  6.47508347e-02]\n",
      " [-5.05614916e-06  2.72284528e-06  6.09987637e-07  2.35404197e-06\n",
      "   8.52353088e-06  9.10218917e-06 -5.52354533e-06 -3.31135210e-10\n",
      "  -4.17139002e-06  3.88179691e-06]\n",
      " [-6.36935292e-02  3.43003380e-02  7.68416123e-03  2.96544339e-02\n",
      "   1.07372972e-01  1.14662470e-01 -6.95814314e-02 -4.17139002e-06\n",
      "  -5.25480052e-02  4.88999310e-02]\n",
      " [ 5.92716921e-02 -3.19190834e-02 -7.15069873e-03 -2.75957149e-02\n",
      "  -9.99187488e-02 -1.06702183e-01  6.47508347e-02  3.88179691e-06\n",
      "   4.88999310e-02 -4.55051194e-02]]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + jnp.exp(-x))\n",
    "\n",
    "def nn(x, weights, b):\n",
    "    return sigmoid(jnp.dot(weights, x) + b)\n",
    "\n",
    "key = jax.random.PRNGKey(100)\n",
    "ws = jax.random.normal(key, shape=(10, ))\n",
    "\n",
    "key, _ = jax.random.split(key)\n",
    "xs = jax.random.normal(key, shape=(10, ))\n",
    "\n",
    "grad_of_nn = jax.grad(nn, argnums=(1))\n",
    "hessian_of_nn = jax.hessian(nn, argnums=(1))\n",
    "\n",
    "print(grad_of_nn(xs, ws, 1.))\n",
    "print(hessian_of_nn(xs, ws, 1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eb666c",
   "metadata": {},
   "source": [
    "### Jacobian-Vector Product (JVP), Vector-Jacobian Product (VJP), and Hessian Vector Product (HVP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd94b1",
   "metadata": {},
   "source": [
    "In a plethora of applications, it is not the Jacobian or Hessian you want to solve, but the matrix-vector product $\\text Jx$ for some Jacobian or Hessian $\\text J$ and a vector $x$. For example, a commonly seen operator in SDE or PDE is $$(A\\phi)(x) = \\nabla_x\\phi \\cdot a(x) + \\frac{1}{2}\\text{tr}\\left( \\Gamma(x) \\text H_x\\phi \\right),$$ and other examples include Gauss-Newton, quasi-Newton methods, and extended Kalman filters, etc. These can be solved efficiently using `jax.vjp` and `jax.jvp`. For details, see [here](https://docs.jax.dev/en/latest/notebooks/autodiff_cookbook.html#how-it-s-made-two-foundational-autodiff-functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c5dd6b",
   "metadata": {},
   "source": [
    "## Control Flows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc60968",
   "metadata": {},
   "source": [
    "Does this work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "53a9b88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1., dtype=float64, weak_type=True)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_if(x):\n",
    "    if x < 0:\n",
    "        return -x\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "test_if(jnp.array(-1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1a54f3",
   "metadata": {},
   "source": [
    "Yes, it works. How about the for loop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ef3f9baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([1., 1.], dtype=float64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_for(x):\n",
    "    for i in range(5):\n",
    "        x = jnp.eye(2) @ x\n",
    "    return x\n",
    "\n",
    "test_for(jnp.ones((2, )))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa5e75f",
   "metadata": {},
   "source": [
    "Yes, it works too! It *seems* a fuss to worry about using normal Python control flows. But, if we want to use `jit` or some autodiff features, you need to be careful with control flows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f9a2fe",
   "metadata": {},
   "source": [
    "For instance, will this work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c72c3561",
   "metadata": {},
   "outputs": [
    {
     "ename": "TracerBoolConversionError",
     "evalue": "Attempted boolean conversion of traced array with shape bool[].\nThe error occurred while tracing the function test_if at /var/folders/n5/bkphn15s0kzc1lsr8x64v8y00000gn/T/ipykernel_1159/330552377.py:1 for jit. This concrete value was not available in Python because it depends on the value of the argument x.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTracerBoolConversionError\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      6\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mtest_if\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1.\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 14 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mtest_if\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;129m@jax\u001b[39m.jit\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_if\u001b[39m(x):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x < \u001b[32m0\u001b[39m:\n\u001b[32m      4\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m -x\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coursework/Macroeconomics/lib/python3.12/site-packages/jax/_src/core.py:1604\u001b[39m, in \u001b[36mconcretization_function_error.<locals>.error\u001b[39m\u001b[34m(self, arg)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merror\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[32m-> \u001b[39m\u001b[32m1604\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m TracerBoolConversionError(arg)\n",
      "\u001b[31mTracerBoolConversionError\u001b[39m: Attempted boolean conversion of traced array with shape bool[].\nThe error occurred while tracing the function test_if at /var/folders/n5/bkphn15s0kzc1lsr8x64v8y00000gn/T/ipykernel_1159/330552377.py:1 for jit. This concrete value was not available in Python because it depends on the value of the argument x.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def test_if(x):\n",
    "    if x < 0:\n",
    "        return -x\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "test_if(jnp.array(-1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a5c7f1",
   "metadata": {},
   "source": [
    "No, the error message explained it well already. Basically, it fails because we are building a computational graph that changes based on the **concrete** calue of a variable. We can force this example to work by using `static_argnums` argument for `jit`. But please use it only if this is intentional, that is, `x` is really static.\n",
    "\n",
    "Recall that `jax.jit` needs to trace all the (numerical) operations to compile to XLA code. `jax.jit` cannot trace the Python control flows, such as `if else` and `for`. If the function to be jitted has a for loop, then the operations in the for loop are hardcoded to the XLA program. Why? Imagine we ask you to implement a function\n",
    "```python\n",
    "def my_jit(f):\n",
    "    return ...\n",
    "```\n",
    "such that `my_jit` takes a function `f` as input and detect whether the function has a for loop. This is super difficult, and we might need to print the function `f` as string then semantically search for the `for`.\n",
    "\n",
    "The following is a more illustrative example. Suppose that JAX can compile a Python code to a C code. How would the compiled C code of\n",
    "```python\n",
    "for i in range(100):\n",
    "    x = f(x)\n",
    "``` \n",
    "look like? We expect to get a C code like this:\n",
    "```c\n",
    "for (int i = 0; i < 100; i++) {\n",
    "    x = f(x);\n",
    "}\n",
    "```\n",
    "But actually, we get\n",
    "```c\n",
    "x = f(x);\n",
    "x = f(x);\n",
    "... // hardcore-repeat 100 times\n",
    "```\n",
    "Hence, if we desire `for` or `if` in the runtime, we need to write something that `jax` could understand and parse, that are, JAX primitives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf93b88",
   "metadata": {},
   "source": [
    "### Conditional Statements and Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbdfc08",
   "metadata": {},
   "source": [
    "Consider a Python *if-else* statement.\n",
    "```python\n",
    "if condition:\n",
    "    result = true_func(x)\n",
    "else:\n",
    "    result = false_func(x)\n",
    "```\n",
    "In JAX, we write as\n",
    "```python\n",
    "result = jax.lax.cond(condition, true_func, false_func, operand=x)\n",
    "```\n",
    "Let us implement `test_if` in JAX as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "417c5f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1., dtype=float64, weak_type=True)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jax.jit\n",
    "def test_if(x):\n",
    "    return jax.lax.cond(x < 0.,       # condition\n",
    "                        lambda _: -x, # what to execute if the condition is true\n",
    "                        lambda _: x,  # what to execute if the condition is false\n",
    "                        x)            # operand here can be anything because we used x from outer scope\n",
    "\n",
    "\n",
    "test_if(jnp.array(-1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3a3c27be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1., dtype=float64, weak_type=True)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jax.jit\n",
    "def test_if(x):\n",
    "    return jax.lax.cond(x < 0.,\n",
    "                        lambda u: -u,\n",
    "                        lambda u: u, \n",
    "                        x)\n",
    "\n",
    "test_if(jnp.array(-1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cab181",
   "metadata": {},
   "source": [
    "Let's try some simple exercise. Write ELU activation function in JAX and JIT it.\n",
    "\n",
    "$$\\text{elu}(x) = \\begin{cases} e^x, & x < 0\\\\ 1, &x \\ge 0\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b0be4317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1., dtype=float64, weak_type=True)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jax.jit\n",
    "def elu(x):\n",
    "    return jax.lax.cond(x < 0.,\n",
    "                        lambda _: jnp.exp(x),\n",
    "                        lambda _: 1.,\n",
    "                        x)\n",
    "\n",
    "# Test\n",
    "elu(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c488c128",
   "metadata": {},
   "source": [
    "What if we have multiple if conditions, i.e., `if elif elif ... else`? Then use `jax.switch`. What if we have vector input? Then use `jnp.where` or `jax.vmap`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17360670",
   "metadata": {},
   "source": [
    "### Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d0d13f",
   "metadata": {},
   "source": [
    "|construct|jit|grad|\n",
    "|----|----|----|\n",
    "|`if`|❌|✅|\n",
    "|`for`|✅*|✅|\n",
    "|`while`|✅*|✅|\n",
    "|`lax.cond`|✅|✅|\n",
    "|`lax.while_loop`|✅|fwd|\n",
    "|`lax.fori_loop`|✅|fwd|\n",
    "|`lax.scan`|✅|✅|\n",
    "\n",
    "\\* means argument-valuable-independent loop condition - unrolls the loop. See [here](https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html)\n",
    "\n",
    "Similar to the `jax.lax.cond` we have seen above, for loops, we have `jax.lax.while_loop`, `jax.lax.fori_loop`, `scan`. Consider a naive numpy implementation of summation:\n",
    "\n",
    "```python\n",
    "def my_sum(x):\n",
    "    summation = 0.\n",
    "    for i in range(x.shape[0]):\n",
    "        summation += x[i]\n",
    "    return summation\n",
    "```\n",
    "\n",
    "The JAX implementation of it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "09ea1128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(10., dtype=float64)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_sum(x):\n",
    "\n",
    "    def body_func(i, val):\n",
    "        return x[i] + val\n",
    "    \n",
    "    return jax.lax.fori_loop(lower=0,            # the starting index\n",
    "                             upper=x.shape[0],   # the number of loops\n",
    "                             body_fun=body_func, # the loop body (index, previous_val) -> val\n",
    "                             init_val=0.)        # initial value of the loop val\n",
    "\n",
    "my_sum(jnp.ones((10, )))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0250f1",
   "metadata": {},
   "source": [
    "The compiled function looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "63164c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f64[10]. let\n",
       "    _:i64[] b:f64[] = scan[\n",
       "      _split_transpose=False\n",
       "      jaxpr={ lambda ; c:f64[10] d:i64[] e:f64[]. let\n",
       "          f:i64[] = add d 1\n",
       "          g:bool[] = lt d 0\n",
       "          h:i64[] = convert_element_type[new_dtype=int64 weak_type=False] d\n",
       "          i:i64[] = add h 10\n",
       "          j:i64[] = select_n g d i\n",
       "          k:f64[1] = dynamic_slice[slice_sizes=(1,)] c j\n",
       "          l:f64[] = squeeze[dimensions=(0,)] k\n",
       "          m:f64[] = convert_element_type[new_dtype=float64 weak_type=False] e\n",
       "          n:f64[] = add l m\n",
       "        in (f, n) }\n",
       "      length=10\n",
       "      linear=(False, False, False)\n",
       "      num_carry=2\n",
       "      num_consts=1\n",
       "      reverse=False\n",
       "      unroll=1\n",
       "    ] a 0 0.0\n",
       "  in (b,) }"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.make_jaxpr(my_sum)(jnp.ones((10, )))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11de3765",
   "metadata": {},
   "source": [
    "Now that if we don't use the JAX language `jax.fori_loop` but simply use Python for loop, what does JAX see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8a6bf553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f64[10]. let\n",
       "    b:f64[1] = slice[limit_indices=(1,) start_indices=(0,) strides=None] a\n",
       "    c:f64[] = squeeze[dimensions=(0,)] b\n",
       "    d:f64[] = add 0.0 c\n",
       "    e:f64[1] = slice[limit_indices=(2,) start_indices=(1,) strides=None] a\n",
       "    f:f64[] = squeeze[dimensions=(0,)] e\n",
       "    g:f64[] = add d f\n",
       "    h:f64[1] = slice[limit_indices=(3,) start_indices=(2,) strides=None] a\n",
       "    i:f64[] = squeeze[dimensions=(0,)] h\n",
       "    j:f64[] = add g i\n",
       "    k:f64[1] = slice[limit_indices=(4,) start_indices=(3,) strides=None] a\n",
       "    l:f64[] = squeeze[dimensions=(0,)] k\n",
       "    m:f64[] = add j l\n",
       "    n:f64[1] = slice[limit_indices=(5,) start_indices=(4,) strides=None] a\n",
       "    o:f64[] = squeeze[dimensions=(0,)] n\n",
       "    p:f64[] = add m o\n",
       "    q:f64[1] = slice[limit_indices=(6,) start_indices=(5,) strides=None] a\n",
       "    r:f64[] = squeeze[dimensions=(0,)] q\n",
       "    s:f64[] = add p r\n",
       "    t:f64[1] = slice[limit_indices=(7,) start_indices=(6,) strides=None] a\n",
       "    u:f64[] = squeeze[dimensions=(0,)] t\n",
       "    v:f64[] = add s u\n",
       "    w:f64[1] = slice[limit_indices=(8,) start_indices=(7,) strides=None] a\n",
       "    x:f64[] = squeeze[dimensions=(0,)] w\n",
       "    y:f64[] = add v x\n",
       "    z:f64[1] = slice[limit_indices=(9,) start_indices=(8,) strides=None] a\n",
       "    ba:f64[] = squeeze[dimensions=(0,)] z\n",
       "    bb:f64[] = add y ba\n",
       "    bc:f64[1] = slice[limit_indices=(10,) start_indices=(9,) strides=None] a\n",
       "    bd:f64[] = squeeze[dimensions=(0,)] bc\n",
       "    be:f64[] = add bb bd\n",
       "  in (be,) }"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This loop is jittable indeed, but ...\n",
    "def my_sum_native(x):\n",
    "    summation = 0.\n",
    "    for i in range(x.shape[0]):\n",
    "        summation += x[i]\n",
    "    return summation\n",
    "\n",
    "jax.make_jaxpr(my_sum_native)(jnp.ones(10, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60318569",
   "metadata": {},
   "source": [
    "See? Again recall that JAX cannot trace the Python control flows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9daa54",
   "metadata": {},
   "source": [
    "Now we practice with some examples. Consider a recursion $$X_t = 0.1 X_{t-1} + U_t, \\quad \\forall t = 1, \\cdots, T.$$ Suppose that the initial $X_0$ and inputs $\\{U_t\\}^{T}_{t=1}$ are known. Compute $X_T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1a446e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.22222222221000001)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple NumPy way of implementation\n",
    "def recursion(x0, us):\n",
    "    T = us.shape[0]\n",
    "    x = x0\n",
    "    for t in range(T):\n",
    "        x = 0.1 * x + us[t]\n",
    "    return x\n",
    "\n",
    "recursion(np.array(0.1), 0.2 * np.ones((10, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "623ff00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.22222222, dtype=float64)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# JAX way of implementation\n",
    "def recursion(x0, us):\n",
    "\n",
    "    def body_func(t, x):\n",
    "        x = 0.1 * x + us[t]\n",
    "        return x\n",
    "\n",
    "    return jax.lax.fori_loop(lower=0,\n",
    "                             upper=us.shape[0],\n",
    "                             body_fun=body_func,\n",
    "                             init_val=x0)\n",
    "\n",
    "recursion(jnp.array(0.1), 0.2 * jnp.ones((10, )))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d16c28",
   "metadata": {},
   "source": [
    "Wait a sec, but the function only returns the value of $X$ at $T$. How do I keep all the history results? This is very simple in NumPy. Just introduce a result accumulator, say, `xs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e401d711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21      , 0.221     , 0.2221    , 0.22221   , 0.222221  ,\n",
       "       0.2222221 , 0.22222221, 0.22222222, 0.22222222, 0.22222222])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recursion(x0, us):\n",
    "    T = us.shape[0]\n",
    "    xs = np.zeros((T, )) # the accumulator\n",
    "\n",
    "    x = x0\n",
    "    for t in range(T):\n",
    "        x = 0.1 * x + us[t]\n",
    "        xs[t] = x\n",
    "    return xs\n",
    "\n",
    "recursion(np.array(0.1), 0.2 * np.ones((10, )))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d53a320",
   "metadata": {},
   "source": [
    "Can I do the same in JAX?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1414987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def recursion(x0, us):\n",
    "    xs = jnp.zeros((T, ))\n",
    "\n",
    "    def fori_body(t, x):\n",
    "        x = 0.1 * x + us[t]\n",
    "        xs[t] = x\n",
    "        return x\n",
    "    \n",
    "    return jax.lax.fori_loop(lower=0,\n",
    "                             upper=us.shape[0],\n",
    "                             body_fun=fori_body,\n",
    "                             init_val=x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6804f04",
   "metadata": {},
   "source": [
    "No. We will get an error in the line `xs[t] = x` because JAX DeviceArray are immutable (i.e., no assignment). We can, to some extent, force `xs[k] = x` to work by using \"JAX array update\" at the cost of making your program nasty, slow, and unreadable. The authentic way to do it is by using the `scan` operation, because $$X_{t} = 0.1 X_{t-1} + U_{t}$$ is essentially a scan operation. Think about what the essential parts of such scan loop are, then we can abstract them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "794052d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursion(x0, us):\n",
    "\n",
    "    def scan_body(carry, elem):\n",
    "        # Unpack carry and elem\n",
    "        x = carry\n",
    "        u = elem\n",
    "\n",
    "        x = 0.1 * x + u\n",
    "        return x, x # scan body returns two values. First returns as the next carry, the second goes to the result container.\n",
    "    \n",
    "    return jax.lax.scan(scan_body, # the scan body function\n",
    "                        x0,        # initial value/carry\n",
    "                        us)        # inputs\n",
    "\n",
    "last_x, xs = recursion(jnp.array(0.1), 0.2 * jnp.ones((10, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "393c7964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.21      , 0.221     , 0.2221    , 0.22221   , 0.222221  ,\n",
       "       0.2222221 , 0.22222221, 0.22222222, 0.22222222, 0.22222222],      dtype=float64)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1707374d",
   "metadata": {},
   "source": [
    "Here is the last exercise. Consider a stochastic differential equation $$dX(t) = \\sin(10 \\pi X(t))dt + dW(t)$$ where $X(0) = 0.1$. Use Euler-Maruyama to simulate a trajectory of $X$ at times $0.01, 0.02, \\cdots, 1$. The formula is $$X(t_k) \\approx X(t_{k-1}) + \\sin(10 \\pi X(t_{k-1}))(t_k - t_{k-1}) + \\Delta W_k, \\quad \\Delta W_k \\sim N(0, t_k - t_{k-1}).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8e018d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x34e7b6630>]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dt = 0.01\n",
    "T = 100\n",
    "ts = jnp.linspace(dt, dt * T, T)\n",
    "\n",
    "key = jax.random.PRNGKey(100)\n",
    "ws = jnp.cumsum(jnp.sqrt(dt) * jax.random.normal(key, (T, ))) # Wiener process at times\n",
    "\n",
    "def scan_body(carry, elem):\n",
    "    x = carry\n",
    "    dw = elem\n",
    "\n",
    "    x = x + jnp.sin(10 * jnp.pi * x) * dt + dw\n",
    "    return x, x\n",
    "\n",
    "_, xs = jax.lax.scan(scan_body, jnp.array(0.1), ws)\n",
    "plt.plot(ts, xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f8ce1c",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5535b93a",
   "metadata": {},
   "source": [
    "- **Sargent, Thomas J. and John Stachurski. n.d.** \"Quantitative Economics with Python using JAX.\" QuantEcon. https://jax.quantecon.org/intro.html\n",
    "- **Zhao, Zheng. 2022.** \"JAX Workshop.\" A Computational Introduction to Stochastic Differential Equations. https://github.com/spdes/computational-sde-intro-lecture/tree/main/lectures/seminar_lectures/zhao_jax_workshop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Macroeconomics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
